{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ネットワーク図を使って内部リンクを可視化するプログラム.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOh5fw06S7ZvOW+NHV00xdg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Charee-Villapong/charee-villapong/blob/master/%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC%E3%82%AF%E5%9B%B3%E3%82%92%E4%BD%BF%E3%81%A3%E3%81%A6%E5%86%85%E9%83%A8%E3%83%AA%E3%83%B3%E3%82%AF%E3%82%92%E5%8F%AF%E8%A6%96%E5%8C%96%E3%81%99%E3%82%8B%E3%83%97%E3%83%AD%E3%82%B0%E3%83%A9%E3%83%A0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PHA67QBpJJMD",
        "outputId": "22a500a3-bcc5-45cc-e8e7-5c99776b4322"
      },
      "source": [
        "!pip install urllib3\n",
        "!pip install networkx\n",
        "!pip install matplotlib\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.6/dist-packages (1.24.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (2.5)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx) (4.4.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (3.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.8.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.4.7)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (1.19.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.1->matplotlib) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-KVy3Ba9WJh"
      },
      "source": [
        "# プログラム終了のためのsysのimport\n",
        "import sys\n",
        "# 正規表現を使うためのreをimport\n",
        "import re\n",
        "# HTTPリクエストを送るためのrequestsをimport\n",
        "import requests\n",
        "# BeautifulSoupをimport\n",
        "from bs4 import BeautifulSoup\n",
        "# URLからドメインを取得する為にurlparseをimport\n",
        "from urllib.parse import urlparse\n",
        "# ネットワーク図を作成する為にnetworkxをimport\n",
        "import networkx as nx\n",
        "# ネットワーク図を描画する為にmatplotlibをimport\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lcXvKwkl-sIp"
      },
      "source": [
        "def main():\n",
        "  \"\"\"\n",
        "  メインの実行部分:調べたいURLはanl_urlに入力する\n",
        "  \"\"\"\n",
        "\n",
        "  # 空のセットを用意\n",
        "  pages = set()\n",
        "  # 内部リンクを調べたいURL\n",
        "  anl_url = \"https://hashikake.com/RegEx\"\n",
        "  # https://またはhttp://からはじまる基準のホームページのURL\n",
        "  match_url = re.match(r\"https?://.*?/\",anl_url)\n",
        "  # match_urlはマッチオブジェクトなので、そこからURLだけを取り出す\n",
        "  base_url = match_url.group()\n",
        "  # 正規表現で使うためにドメイン名の取得\n",
        "  base_domain = urlparse(anl_url).netloc\n",
        "  #　内部リンクの取得\n",
        "  inner_links = get_links(anl_url,pages,base_url,base_domain)\n",
        "  # 内部リンクが存在するなら\n",
        "  if inner_links:\n",
        "    print(f\"価値が高い内部リンクは全部で{len(inner_links)}個あります\")\n",
        "    # 内部リンクの数を表示\n",
        "    print(inner_links)\n",
        "     # 内部リンクの表示\n",
        "\n",
        "    # 関数内で使われるshort_linksを定義\n",
        "    short_links = shape_url(inner_links,base_url,base_domain) \n",
        "    # 内部リンクとして価値の薄いものの除外\n",
        "    print(f'価値が高い内部リンクは全部で{len(short_links)}個あります')\n",
        "    # 内部リンクとして価値が高いものの数を表示\n",
        "    show_network(short_links)\n",
        "  # 内部リンクが存在しない場合はプログラムの終了\n",
        "  else:\n",
        "    print('内部リンクを取得できませんでした')\n",
        "    sys.exit()\n",
        "\n",
        "\n",
        "\n",
        "def get_links(anl_url,pages,base_url,base_domain):\n",
        "  \"\"\"\n",
        "  /で始まるものと、base_urlから始まるもの//ドメインから始まるもの\n",
        "  全ての内部リンクを取得して、重複を除去してpagesに収集する＋\n",
        "  内部リンク数を出力\n",
        "  \"\"\"\n",
        "      \n",
        "  # 正規表現の中で変数を使う時はf文字列またはformatを使う\n",
        "  # /で始まって//を含まないURLと、https://ドメインから始まるもの、//ドメインから始まるもの\n",
        "  pattern = rf\"^{base_url}.*|^/[^/].*|^//{base_domain}.*\"\n",
        "  # URLにGETリクエストを送る\n",
        "  response = requests.get(anl_url)\n",
        "  # BeautifulSoupによるsoupの作成\n",
        "  soup = BeautifulSoup(response.content,\"html.parser\")\n",
        "  # URLがパターンに一致するaタグを取得\n",
        "  for link in soup.find_all('a' , href=re.compile(pattern)):  # re.compileによる正規表現パターンの生成\n",
        "    link.get('href')   #aタグの中からurlを取得\n",
        "\n",
        "  # セットの中にリンクが入っていないことを確認\n",
        "    if link.get('href') not in pages:\n",
        "  # セットの中に内部リンクとして追加\n",
        "      pages.add(link.get('href'))\n",
        "  return pages\n",
        "\n",
        "def show_network(pages):\n",
        "  \"\"\"\n",
        "  調査URLを中心としたネットワーク図の作成\n",
        "  \"\"\"\n",
        "\n",
        "  # セットをリストにする\n",
        "  pages = list(pages)\n",
        "  # indexの0に文字列\"start_url\"を追加\n",
        "  pages.insert(0,\"start_url\")\n",
        "  #　空のグラフの作成　有向グラフ\n",
        "  G = nx.DiGraph()\n",
        "  # リストの最初の要素を中心として放射状に頂点と辺の追加\n",
        "  nx.add_star(G,pages)\n",
        "  # レイアウトを決める スプリングレイアウト\n",
        "  pos = nx.spring_layout(G,k=0.3)\n",
        "  # ノードの様式の決定\n",
        "  nx.draw_networkx_nodes(G,pos,node_size=300,node_color='c',alpha=0.6)\n",
        "  # ラベル文字の様式の決定\n",
        "  nx.draw_networkx_labels(G,pos,font_size=10,font_family='DejaVu Sans')\n",
        "  # エッジの様式の決定\n",
        "  nx.draw_networkx_edges(G,pos,alpha=0.4,edge_color='c')\n",
        "  #nx.draw_networkx(G, pos)\n",
        "\n",
        "  # matplotlibの座標軸の非表示\n",
        "  plt.axis('off')\n",
        "  # matplotlibによる図の描画\n",
        "  plt.show()\n",
        "\n",
        "def shape_url(inner_links,base_url,base_domain):\n",
        "  \"\"\"\n",
        "  URLのhttp://を省略してネットワーク図を見やすくするための調整\n",
        "  privacyページやcontactページなどの無駄な内部リンクページの除去\n",
        "  \"\"\"    \n",
        "  short_links = []\n",
        "  # 内部リンクのURLから効果の薄い内部リンクをre.sub()で消していく base_url https://hashikake.com //hashikake.com\n",
        "  for url in pages:\n",
        "    rel_path = re.sub(rf\"^{base_url}|^//{base_domain}|.*tag.*|.*feed.*|.*about.*\",\"\",url)\n",
        "  # short_links（空のリスト）に追加\n",
        "    short_links.append(rel_path)\n",
        "  # short_linksをセットに変更(重複の削除)\n",
        "    s_links = set(short_links)\n",
        "  # \"\"を削除　# discardだとキーがなくてもエラーにはならない。removeだとエラーになる\n",
        "    print(s_link)\n",
        "    return s_link\n",
        "\n",
        "if __name__ == 'main':\n",
        "    main()\n"
      ],
      "execution_count": 20,
      "outputs": []
    }
  ]
}